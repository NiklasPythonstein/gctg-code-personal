{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze-controlled text generation\n",
    "\n",
    "This notebook demonstrates how to generate texts with the language model / gaze model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from modeling.gaze_models import CausalTransformerGazeModel\n",
    "from modeling.generation import GazeControlledBeamSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "The texts in our experiment were generated using the off-the-shelf instruction-tuned Llama-3.2 language model with 3B parameters. The gaze model is a GPT-2 model fine-tuned to predict first-pass gaze duration.\n",
    "\n",
    "> **NOTE:** Expect the ensemble to be quite slow on CPU (up to a minute per token), so you should consider either using a GPU (e.g., on Google Colab) or choosing smaller model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGIN\n",
    "from huggingface_hub import login\n",
    "login(token=\"hf_xxxxxxxx\")\n",
    "\n",
    "#Preemptive pytorch fix (that doesnt work)\n",
    "#import os\n",
    "#os.environ['TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m language_model = AutoModelForCausalLM.from_pretrained(language_model_name).to(device)\n\u001b[32m      9\u001b[39m gaze_model = CausalTransformerGazeModel.from_pretrained(gaze_model_name).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m gaze_model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/trf_gaze_model.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/gctg-code-personal/.venv/lib/python3.12/site-packages/torch/serialization.py:1495\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1493\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1494\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/gctg-code-personal/.venv/lib/python3.12/site-packages/torch/serialization.py:1744\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33mreadinto\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m0\u001b[39m) <= sys.version_info < (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1739\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReceived object of type \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunctionality.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m magic_number = \u001b[43mpickle_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number != MAGIC_NUMBER:\n\u001b[32m   1746\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid magic number; corrupt file?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnpicklingError\u001b[39m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "language_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "gaze_model_name = \"openai-community/gpt2\"\n",
    "\n",
    "language_model_tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "language_model = AutoModelForCausalLM.from_pretrained(language_model_name).to(device)\n",
    "\n",
    "gaze_model = CausalTransformerGazeModel.from_pretrained(gaze_model_name).to(device)\n",
    "gaze_model.load_state_dict(torch.load(\"models/trf_gaze_model.pt\", map_location=device, weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the story prompts\n",
    "\n",
    "The titles and prompts for the stories were generated using GPT-4 and manually curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stories/prompts.jsonl\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "pprint(prompts, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the texts\n",
    "\n",
    "Before starting the generation, we need to build an instruction prompt according to the template that is specific to Llama-3.2. We then generate text using [beam search](https://en.wikipedia.org/wiki/Beam_search) until one of two conditions applies:\n",
    "\n",
    "- the language model has predicted an end-of-message token in the best beam, or\n",
    "- the number of generated tokens has reached 800.\n",
    "\n",
    "![Visualization of beam search with beam size 3](https://upload.wikimedia.org/wikipedia/commons/2/23/Beam_search.gif)\n",
    "\n",
    "Refer to [`generation.py`](modeling/generation.py) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_weight = 2\n",
    "beam_size = 8\n",
    "\n",
    "beam_search = GazeControlledBeamSearch(\n",
    "    language_model,\n",
    "    language_model_tokenizer,\n",
    "    gaze_model,\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "for prompt in prompts:\n",
    "    input_text = language_model_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Write a short story based on the following title and prompt.\\n\"\n",
    "                    f\"Title: {prompt['title']}\\n\"\n",
    "                    f\"Prompt: {prompt['prompt']}\\n\\n\"\n",
    "                    \"The story should not be longer than 500 words. \"\n",
    "                    \"Keep in mind that the reader will not see the prompt, only the story itself. \"\n",
    "                    \"Do not include the title.\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    output_text, token_score, gaze_score = beam_search.generate(\n",
    "        input_text,\n",
    "        gaze_weight=gaze_weight,\n",
    "        max_length=800,\n",
    "        beam_size=beam_size,\n",
    "        ignore_prompt=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    outputs.append(\n",
    "        {\n",
    "            **prompt,\n",
    "            \"gaze_weight\": gaze_weight,\n",
    "            \"input_text\": input_text,\n",
    "            \"output_text\": output_text,\n",
    "            \"token_score\": token_score,\n",
    "            \"gaze_score\": gaze_score,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the outputs\n",
    "\n",
    "This includes the final text from the best beam as well as the total token score from the language model and the gaze score from the gaze model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stories/outputs.jsonl\", \"w\") as f:\n",
    "    for output in outputs:\n",
    "        f.write(json.dumps(output) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
